<h4>An in-depth summary of Facebookâ€™s amazing Vision Transformer DINO</h4> 

<p>DINO, a new self supervised system by Facebook AI, is able to learn incredible representations from unlabeled data. Below is a video visualising 
itâ€™s attention maps and we see the model was able to automatically learn class-specific features leading to accurate unsupervised object segmentation. 
It was introduced in their paper <em>â€œEmerging Properties in Self-Supervised Vision Transformersâ€</em></p> 

<a href="https://medium.com/media/5f27796f3cc06199af8e374f4190de9e/href">https://medium.com/media/5f27796f3cc06199af8e374f4190de9e/href</a>

<p>Hereâ€™s a summary of how it works ğŸ‘‡</p> <blockquote class="twitter-tweet" data-conversation="none" data-align="center" data-dnt="true">
<p>â€Šâ€”â€Š<a href="https://twitter.com/rahuld3eora/status/1394728088429240321">@rahuld3eora</a></p></blockquote> <p><strong><em>TLDR; 
</em></strong><em>A Student ViT learns to predict global features in an image from local patches supervised by the cross entropy loss from a momentum 
Teacher ViTâ€™s embeddings while doing centering and sharpening to prevent mode collapse</em></p> <h3><strong>Networks:</strong></h3> <p>The network learns 
through a process called â€˜self-distillationâ€™. There is a teacher and student network both having the same architecture, a <strong>Vision Transformer(ViT)
</strong>.</p> <p>The teacher is a <strong>momentum teacher</strong> which means that itâ€™s weights are an exponentially weighted average of the studentâ€™s. 
  The momentum teacher was introduced in the paper â€œ<em>Momentum Contrast for Unsupervised Visual Representation Learningâ€</em> in order to prevent mode collapse when the teacher and the student are the same and output the same embeddings regardless of the input.</p> <p>The update rule for the teacherâ€™s weights are:</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/512/1*UnDm6ZOQVhQzzgSuf2CR4w.png"><figcaption>Equation 1: Update rule where Î¸t and Î¸s are the teacherâ€™s and studentâ€™s weights(<a href="https://arxiv.org/abs/2104.14294">Source</a>)</figcaption></figure><p>with Î» following a <strong>cosine schedule from 0.996 to 1</strong> during training in this paper.</p> <h3>Data:</h3> <p>As is common in self-supervised learning, different crops of one image are taken. Small crops are called Local views( <50% of the image) and large crops( >50% of the image) are called Global views.</p> <p>All crops are passed through the student while only the global views are passed through the teacher. <strong><em>This encourages â€œlocal-to-globalâ€ correspondence, training the student to interpolate context from a small crop.</em></strong> See Fig 1.</p> <p>Random augmentations of color jittering, Gaussian blur and solarization are also applied on the views to make the network more robust.</p> <h3>Loss</h3> <p>The teacher and student each predict a 1-dimensional embedding. A softmax along with cross entropy loss is applied to make studentâ€™s distribution match the teacherâ€™s</p> <p>Softmax is like a normalisation, it converts the raw activations to represent how much each feature was present relative to the whole. eg) [-2.3, 4.2, 0.9 ,2.6 ,6] ->[0.00 , 0.14, 0.01, 0.03, 0.83] so we can say the last featureâ€™s strength is 83% and we would like the same in the studentâ€™s as well. <strong><em>So we are asking our student network to have the same proportions of features as the teacher.</em></strong> <strong><em>The teacher having a larger context predicts more high level features which the student must also match.</em></strong></p> <p>The cross-entropy loss tries to make the two distributions the same just as in knowledge distillation.</p> <p>This can also be seen as a made up classification problem. <strong><em>We are asking our network to make up a classification problem such that the network can learn meaningful global representations from local views.</em></strong></p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/632/1*C1_JK8uZIg_zCapv9d0ITA.png"><figcaption>Fig 2: DINO Flow(<a href="https://arxiv.org/abs/2104.14294">Source</a>)</figcaption></figure><h3>Centering and Sharpening</h3> <p>There are two forms of mode collapse: regardless of the input, the model output is always the same along all the dimensions(i.e same output for any input) or dominated by one dimension. Centering and Sharpening aim to prevent both these.</p> <p><strong>Centering</strong>: The teacherâ€™s raw activations have the their exponentially moving average subtracted from them. It simply is:</p> <p><em>Logits = Logits - Logits_mean</em></p> <p><strong><em>This means that activations must be sometimes positive when they are above their mean and sometimes negative when they are below.</em></strong> This prevents any one feature from dominating as the mean will be somewhere in the middle of the range. And we know that softmax gives very low values to negative numbers and high values to positive ones.</p> <p><strong>Sharpening<em>: Sharpening is the same as applying a temperature to the softmax to artificially make the distribution more peaked</em></strong>, i.e exaggerate small differences so that there is one or some high values and some low values. This prevents all the activations from being the same value as small differences are exaggerated. This acts in synergy with centering which keeps changing which activations are high. Sharpening also helps the student get a stronger signal which features it should increase.</p> <p>DINO Psedudocode:</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/948/1*kUkf2ilf5KySJsdX9RcWQg.png"><figcaption>Fig 3: DINO Psedudocode(<a href="https://arxiv.org/abs/2104.14294">Source</a>)</figcaption></figure><h3>Visualisations:</h3> <p>Here are some attention maps showing that DINO is able to focus exclusively on objects of interest in the image. This means that DINO has understood object semantics so well that itâ€™s attention maps look like segmentation masks.</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*PWhJqJFo5DIDYTuXLsUySg.png"><figcaption>Fig 4: Attention maps(<a href="https://arxiv.org/abs/2104.14294">Source</a>)</figcaption></figure><p>We also find that the latent space of DINO has well separated categories even within animal groups which means that itâ€™s features are rich enough to separate small differences in objects. This makes it very good for downstream tasks and transfer learning.</p> <p>It also does well in recognising duplicate images as shown below.</p> <blockquote class="twitter-tweet" data-conversation="none" data-align="center" data-dnt="true"><p>â€Šâ€”â€Š<a href="https://twitter.com/schrep/status/1388189687953584128">@schrep</a></p></blockquote> <h3>Summary:</h3> <p>A Student ViT learns to predict global features in an image from local patches supervised by the cross entropy loss from a momentum Teacher ViTâ€™s embeddings while doing centering and sharpening to prevent mode collapse â€¦. Wow thats a lot of jargon!</p> <p>â€”â€Šâ€”â€Šâ€”â€Šâ€”â€Šâ€”â€Šâ€”â€Šâ€”â€Šâ€”â€Šâ€”â€Šâ€”â€Šâ€”â€Šâ€”â€Šâ€”â€Šâ€”â€Šâ€”â€Šâ€”â€Šâ€”â€Šâ€”â€Šâ€”â€Šâ€”â€Šâ€”â€Šâ€”â€Šâ€”â€Šâ€”â€Šâ€” â€”</p> <p>Relevant Links for Emerging Properties in Self-Supervised Vision Transformers:</p> <p>Blog: <a href="https://ai.facebook.com/blog/dino-paws-computer-vision-with-self-supervised-transformers-and-10x-more-efficient-training/">https://ai.facebook.com/blog/dino-paws-computer-vision-with-self-supervised-transformers-and-10x-more-efficient-training/</a></p> <p>GitHub: <a href="https://github.com/facebookresearch/dino">https://github.com/facebookresearch/dino</a></p> <p>Paper: <a href="https://arxiv.org/abs/2104.14294">https://arxiv.org/abs/2104.14294</a></p> <p><a href="https://twitter.com/schrep">Mike Schroepfer</a>â€™s tweet: <a href="https://twitter.com/schrep/status/1388189398496202752">https://twitter.com/schrep/status/1388189398496202752</a></p> <p>Yannic: <a href="https://www.youtube.com/watch?v=h3ij3F3cPIk&t=2106s">https://www.youtube.com/watch?v=h3ij3F3cPIk&t=2106s</a></p> <img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=ab91df82cc3c" width="1" height="1" alt=""><hr> <p><a href="https://towardsdatascience.com/dino-emerging-properties-in-self-supervised-vision-transformers-summary-ab91df82cc3c">DINO: Emerging Properties in Self-Supervised Vision Transformers Summary</a> was originally published in <a href="https://towardsdatascience.com/">Towards Data Science</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>
